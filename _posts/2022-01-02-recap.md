---
title: "Recap"
date: 2022-01-02
categories:
  - Blog
tags:
  - NLP
---

### SVM

Kernel trick $K(w,x))$: replace dot-product (similarity) in linear space (wx) to higher dim space, without actually mapping each $w$/$x$.
* [Polynomial kernel](https://en.wikipedia.org/wiki/Polynomial_kernel)
* [RBF](https://en.wikipedia.org/wiki/Radial_basis_function_kernel): $\mathrm{exp}(-\gamma \Vert w-x \Vert^2)$

Solver: convex optimization (small data) or SGD (approx)
