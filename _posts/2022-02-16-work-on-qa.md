---
title: "Work on QA"
date: 2022-02-16
categories:
  - Blog
tags:
  - NLP
  - QA

---

## Synthetic Generation for Augmentation or Domain Transfer

**Synthetic QA Corpora Generation with Roundtrip Consistency**. Alberti et al. ACL'19\
Data augmentation: introduce 3-stage question-answer generation: (1) generate answers unconditional to question: $p(a|c)$ (2) generate questions condition on answers: $p(q | c, a)$ (3) filtration on generated QA pairs to ensure roundtrip consistency.\
<https://www.aclweb.org/anthology/P19-1620>

**Training Question Answering Models From Synthetic Data**. Puri et al. EMNLP'20\
Data augmentation: following Alberti'19 but using better language models and tuning, models trained on only synthetic corpus achieves better performance than human-labeled data on SQuAD.\
<https://www.aclweb.org/anthology/2020.emnlp-main.468>

**End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems**. Shakeri et al. EMNLP'20\
Data augmentation: introduce end-to-end synthetic QA generation without roundtrip filtering, that generates questions and answers jointly: $p(q, a | c)$, and then keep the top m results by their likelihood scores as filtration.\
<https://www.aclweb.org/anthology/2020.emnlp-main.439>

**Contrastive Domain Adaptation for Question Answering using Limited Text Corpora**. Yue et al. EMNLP'21\
Common for QA domain adaptation or semi-supervised learning: e2e synthesis generation of source/target domain + QA on mixed source & target domain.\
Difference of this work: design contrastive loss to distinguish two classes (answer tokens) and (other tokens), and pull together within class representation, inside batch of mixed domains, so that to push domain-agnostic representation.\
<https://aclanthology.org/2021.emnlp-main.754>

## Extractive/Multi-Choice QA

**A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction**. Niu et al. ACL'20\
<https://www.aclweb.org/anthology/2020.acl-main.361/>

## Conversational QA

**A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC**. Yatskar. NAACL'19\
Dataset comparison; BiDAF baseline.\
<https://www.aclweb.org/anthology/N19-1241/>

**An Empirical Study of Content Understanding in Conversational Question Answering**. Chiang et al. AAAL'20\
<https://arxiv.org/abs/1909.10743>

**Do not let the history haunt you: Mitigating Compounding Errors in Conversational Question Answering**. Mandya et al. LREC'20\
<https://www.aclweb.org/anthology/2020.lrec-1.248/>

## Natural Questions (NQ)

**Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension**. Zheng et al. ACL'20\
Hierarchical representation of a document. Sequence encoder for each level: self-attention; aggregation between each level: graph attention network.\
<https://www.aclweb.org/anthology/2020.acl-main.599/>

**RikiNet: Reading Wikipedia Pages for Natural Question Answering**. Liu et al. ACL'20\
<https://www.aclweb.org/anthology/2020.acl-main.604/>

## Multi-Hop Reasoning

TODO

## Open-Retrieval QA

**SituatedQA: Incorporating Extra-Linguistic Contexts into QA**. Zhang and Choi. EMNLP'21\
Temporal/geographical context-dependent QA.\
<https://aclanthology.org/2021.emnlp-main.586/>
